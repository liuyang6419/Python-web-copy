使用Tensorflow构建与训练神经网络非常快速与便捷，神经网络的构建与训练分别用到了`nn`模块与`train`模块。



## 1. NN模块

`nn`模块可以用来方便的构建人工神经网络。模块包含了激活函数、卷积函数、池化函数、分类器、形态学滤波方法、正则化方法、损失度量方法、循环神经网络单元、评估方法等。这些方法为我们构建神经网络提供了强大的支持。

这里我们从构建线性回归模型起，利用NN模型以及相关方法依次构建线性回归模型、逻辑回归模型、全连接神经网络模型、卷积神经网络模型。以此来了解NN模块。



### 1.1 构建线性回归模型

线性回归模型可以用来拟合线性分布数据模型，其假设函数为$h_\theta(x)=\theta^TX$，这里$\theta$表示线性回归模型的参数。

线性回归模型是最简单的机器学习模型，这里我们假设输入$X$为一个样本的输入，其shape为$(2, )$，使用Tensorflow构建模型的方法如下：

~~~python
import tensorflow as tf

# 假设模型输入为`X`
X = tf.placeholder(shape=[2, ])

# 假设参数theta包含权重`W`与偏置值`b`两部分
# `W`使用随机数初始化 `b`使用0初始化
W = tf.Variable(tf.random_normal(shape=[2, ]))
b = tf.Variable(tf.zeros(shape=[]))

h = tf.reduce_sum(tf.multiply(X, W)) + b
~~~

测试模型：

~~~python
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(h, feed_dict={X: [1, 2]})
~~~



#### 多样本线性回归

事实上，为了训练模型方便，通常的，模型必须能够同时处理多个样本。这时候，使用Tensorflow构建模型的方法如下（一个样本的输入shape依然为$(2, )$）：

~~~python
import tensorflow as tf

batch_num = 3
# 假设模型输入为`X` 此处的`X`为`batch_num`个样本
X = tf.placeholder(shape=[batch_num, 2], dtype=tf.float32)

# `W`与`b`同上
W = tf.Variable(tf.random_normal(shape=[2, 1]))
b = tf.Variable(tf.zeros(shape=[]))

h = tf.matmul(X, W) + b
~~~

测试模型：

~~~python
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(h, feed_dict={X: [[1, 2], [3, 4], [5, 6]]}))
~~~



### 1.2 构建逻辑回归模型

逻辑回归模型本质上是一个二分类模型，而非回归模型，逻辑回归可以看做是线性回归模型在分类问题上的推广，即在线性回归模型之后接上logistic函数，进行二分类。为了使逻辑回归拥有更加强大的拟合能力，我们可以将输入logistic的线性部分替换为多项式的形式。逻辑回归是一个强大的分类器。

这里我们首先讨论最简单的逻辑回归模型。其假设函数的形式为$h=\frac{1}{1+e^{-\theta^TX}}$，其中$\theta$代表模型参数。当$h>0.5$时，我们认为其输出为正类，否则输出为负类。

使用Tensorflow构建此模型的方法如下（基于多样本）：

~~~python
import tensorflow as tf

batch_num = 3
X = tf.placeholder(shape=[batch_num, 2], dtype=tf.float32)

W = tf.Variable(tf.random_normal(shape=[2, 1]))
b = tf.Variable(tf.zeros(shape=[]))

z = tf.matmul(X, W) + b
h = tf.nn.sigmoid(z)
~~~

这里，我们使用了`tf.nn.sigmoid`方法，其作用就是将$z$利用logistic函数进行变换。其输入shape与输出shape相同。

测试模型：

~~~python
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(h, feed_dict={X: [[1, 2], [3, 4], [5, 6]]}))
~~~



### 1.3 构建全连接神经网络

全连接神经网络是以层为单位构建的，每一层包含一定数量的神经元，相邻两层之间采用全连接的方式传递信息，即前一层的每一个输出（或者为输入层的样本输入）均与当前层的每一个神经元相连接，连接的强度，我们使用参数w来表示，每两层之间均有这样的参数。神经网络的输出层需要根据任务的类型确定，所其既可以做分类任务，又可以做回归任务。当做分类任务时，输出层通常使用softmax输出或者logistic输出。要注意的是logistic输出只能用来做二分类；当做回归任务时，其输出层通常使用线性输出单元。

这里我们以二分类任务为例，假设神经网络中神经元的激活函数都为logistic函数，使用Tensorflow构建全连接神经网络（基于多样本）的方法如下：

**神经网络无隐藏层时，等价于逻辑回归：**

~~~python
import tensorflow as tf

batch_num = 3
X = tf.placeholder(shape=[batch_num, 2], dtype=tf.float32)

W = tf.Variable(tf.random_normal(shape=[2, 1]))
b = tf.Variable(tf.zeros(shape=[]))

z = tf.matmul(X, W) + b
h = tf.nn.sigmoid(z)
~~~



**神经网络有一个隐藏层时：**

~~~python
import tensorflow as tf

batch_num = 3
X = tf.placeholder(shape=[batch_num, 2], dtype=tf.float32)

# 隐藏层
hidden_num = 10
hidden_W = tf.Variable(tf.random_normal(shape=[2, hidden_num]))
hidden_b = tf.Variable(tf.zeros(shape=[hidden_num, ]))
hidden_z = tf.matmul(X, hidden_W) + hidden_b
hidden_output = tf.nn.sigmoid(hidden_z)

# 输出层
output_W = tf.Variable(tf.random_normal(shape=[hidden_num, 1]))
output_b = tf.Variable(tf.zeros(shape=[1, ]))
output_z = tf.matmul(hidden_output, output_W) + output_b
h = tf.nn.sigmoid(output_z)
~~~



多个隐藏层的构建与此类似，只需要增加隐藏层即可，为了方便构建多个隐藏层，这里我们借助函数进行：

~~~python
import tensorflow as tf

batch_num = 3
X = tf.placeholder(shape=[batch_num, 2], dtype=tf.float32)

def build_layer(input_x, num_of_layer, activation_fun=tf.nn.sigmoid):
    W = tf.Variable(
      tf.random_normal(
        shape=[input_x.shape.as_list()[1], num_of_layer]))
    b = tf.Variable(tf.zeros(shape=[num_of_layer, ]))
    return activation_fun(tf.matmul(input_x, W) + b)


hidden_1 = build_layer(X, 10)
hidden_2 = build_layer(hidden_1, 50)
h = build_layer(hidden_2, 1, tf.nn.sigmoid)
~~~

利用上述方法可以方便的构建多层神经网络，例如，我们需要构建一个5层的10分类神经网络，构建方法如下：

~~~python
batch_num = 3
X = tf.placeholder(shape=[batch_num, 2], dtype=tf.float32)

hidden_1 = build_layer(X, 10)
hidden_2 = build_layer(hidden_1, 50)
hidden_3 = build_layer(hidden_2, 50)
hidden_4 = build_layer(hidden_3, 50)
h = build_layer(hidden_4, 10, tf.nn.softmax)
~~~

这里，我们使用了`tf.nn.softmax`方法来对输入进行多分类。使用方法与`tf.nn.sigmoid`类似。



### 1.4 多种激活函数

除了上述提到了两种激活函数外，TensorFlow提供了丰富的激活函数来供我们使用。主要包括以下两类：

* 光滑的非线性激活函数。
* 连续但非处处可微激活函数。

光滑的非线性激活函数主要包括：`sigmoid`, `tanh`, `elu`, `selu`, `softplus`, `softsign`。

连续但非处处可微激活函数主要包括：`relu`, `relu6`, `crelu` , `relu_x`。

其用法大致相同，不在此处赘述。



### 1.5 构建卷积神经网络 

卷积神经网络的构建主要用到了卷积与池化等技术。Tensorflow提供了丰富的卷积与池化方法。例如在卷积上，有对不同维度数据的卷积支持、数学上卷积的支持、带孔卷积等；在池化上支持最大池化、平均池化等。

这里我们以对图片进行卷积为例，需要使用的是2D卷积与池化。需要注意的是，这里我们的输入样本，每个样本张量的rank都是3，即样本shape为[height, width, channles]。当我们构建模型时，通常使用的是批量样本输入，所以模型输入的shape一般为[batch_size, height, width, channles]。对于灰度图像而言channles为1，对于jpg图像而言channles为3。

TensorFlow中的卷积函数拥有较多的参数，这里首先了解下2D卷积函数的内容：

~~~python
tf.nn.conv2d(
    input,  # 输入张量(被卷积对象) , 4D Tensor
    filter,  # 卷积核 [filter_height, filter_width, in_channels, out_channels]
    strides,  # 步长 对输入张量的每一个维度设置步长
    padding,  # 边界处理 `"SAME" 或 "VALID"`
    use_cudnn_on_gpu=True, 
    data_format='NHWC',  # 输入数据格式'NHWC'或'NCHW'
    name=None)
~~~

类似于2D卷积，池化也拥有滑动窗口、步长、边界处理等参数，例如最大池化：

~~~python
tf.nn.max_pool(
    value,  # 输入张量(被卷积对象) , 4D Tensor
    ksize,  # 窗口大小 shape=(4, )
    strides,  # 步长 对输入张量的每一个维度设置步长
    padding,  # 边界处理 `"SAME" 或 "VALID"`
    data_format='NHWC',  # 输入数据格式'NHWC'或'NCHW'
    name=None)
~~~



使用TensorFlow构建卷积神经网络通常需要创建卷积层、池化层，最后将池化后的结果展开，接上全连接层，具体如下：

~~~python
import tensorflow as tf

batch_num = 5
X = tf.placeholder(shape=[batch_num, 28, 28, 3], dtype=tf.float32)

# 卷积层参数
filter_W = tf.Variable(tf.random_normal([3, 3, 3, 5]))
# 卷积 输出的shape=[batch_num, 26, 26, 5]
conv = tf.nn.conv2d(X, filter_W, strides=[1, 1, 1, 1], padding="VALID", )
# 激活函数处理
conv_out = tf.nn.relu(conv)

# 最大池化 输出shape=[batch_num, 13, 13, 5]
maxpool_out = tf.nn.max_pool(conv_out, [1, 2, 2, 1], [1, 2, 2, 1], padding="VALID")
# 将池化后的张量展开
maxpool_out_flat = tf.reshape(maxpool_out, [-1, 13 * 13 * 5])

# 接入全连接层 此处全连接层
fc_W = tf.Variable(tf.random_normal(shape=[13 * 13 * 5, 100]))
fc_b = tf.Variable(tf.zeros(shape=(100, )))
fc_out = tf.nn.relu(tf.matmul(maxpool_out_flat, fc_W) + fc_b)

# 输出层 此处输出由10个单元构成 使用softmax函数做激活函数
h_W = tf.Variable(tf.random_normal(shape=[100, 10]))
h_b = tf.Variable(tf.zeros(shape=(10, )))
h = tf.nn.softmax(tf.matmul(fc_out, h_W) + h_b)
~~~



## 2. train模块

train模块提供了一些列训练模型的类与方法。模块主要包含了优化器、梯度计算方法、数据裁剪、学习率衰减、滑动平均、协调器与队列、分布式执行、训练工具、训练钩子等。其中协调器与队列的部分在前面的章节中已经涉及到了，这里我们主要讲解优化器部分。



### 2.1 优化器

优化器基类`tf.train.Optimizer`有计算代价函数梯度与将梯度应用到更新代价函数中的变量上的作用。其子类则实现了一些经典的最优化算法，利用这些算法可以快捷训练我们的算法。这里我们首先介绍一下最常用的经典的优化算法的使用方法。

* `tf.train.GradientDescentOptimizer`：梯度下降优化器。
* `tf.train.MomentumOptimizer`：动量优化器，其模拟物体运动时的惯性，在GD的梯度更新时增加动量（上一次更新的梯度），降低对当前batch对梯度的影响。
* `tf.train.AdagradOptimizer`：自适应梯度优化器，其针对不同参数应用不同的学习率。[论文](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)。
* `tf.train.AdadeltaOptimizer`：自适应学习率优化器，改进了Adagrad，使得其对初识学习率不敏感。[论文](https://arxiv.org/pdf/1212.5701.pdf)。
* `tf.train.AdamOptimizer`：自适应据估计优化器。[论文](https://arxiv.org/pdf/1412.6980)

其中Adam优化方法是最常用、最有效的一阶优化算法。通常使用Adam算法并不需要手动设置学习率等参数就能获得很好的结果。推荐在深度学习算法中使用Adam优化法。

用法如下：

~~~python
# 创建adam优化器
optimizer = tf.train.AdamOptimizer()

# 最小化目标(loss)
opt = optimizer.minimize(loss)
~~~



### 3. 模型的构建与训练

通过上述内容的了解，我们知道可以使用nn模块构建神经网络模型，使用train模块训练模型，但需要注意的是，在训练之前需要构建代价函数，TensorFlow也提供了一些方法方便使用。这里我们给一个完整的例子来说明两个模块一起使用的方法。

~~~python
# 样本特征与标记占位符
x = tf.placeholder(shape=[None, 28, 28, 3], dtype=tf.float32)
y = tf.placeholder(shape=[None, 10], dtype=tf.float32)

# 构建CNN
# 第一个卷积层
conv1_w = tf.Variable(tf.random_normal([5, 5, 3, 12]))
conv1_b = tf.Variable(tf.zeros([12, ]))
conv1 = tf.nn.relu(tf.nn.conv2d(x, conv1_w, [1, 1, 1, 1], 'VALID') + conv1_b)
# 第一个池化层
pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')
# 第二个卷积层
conv2_w = tf.Variable(tf.random_normal([5, 5, 12, 64]))
conv2_b = tf.Variable(tf.zeros([64, ]))
conv2 = tf.nn.relu(tf.nn.conv2d(pool1, conv2_w, [1, 1, 1, 1], 'VALID') + conv2_b)
# 第二个池化层
pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')
# 展开
flat = tf.reshape(pool2, [-1, 4 * 4 * 64])
# 全连接层
fc3_w = tf.Variable(tf.random_normal([4 * 4 * 64, 100]))
fc3_b = tf.Variable(tf.zeros([100, ]))
fc3 = tf.nn.relu(tf.matmul(flat, fc3_w) + fc3_b)
# 输出层，这里输出logits
fc4_w = tf.Variable(tf.random_normal([100, 10]))
fc4_b = tf.Variable(tf.zeros([10, ]))
logits = tf.matmul(fc3, fc4_w) + fc4_b
# 输出 Tensorflow中为了更高效的处理数据，
# 在训练时，不使用tf.nn.softmax作为输出
# h = tf.nn.softmax(logits)

# 输出交叉熵
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)
# 计算平均代价
loss = tf.reduce_mean(cross_entropy)

# 训练op
optimizer = tf.train.AdamOptimizer()
train_op = optimizer.minimize(loss)


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # 训练100次
    # 此处填充x, y的数据为假数据
    for i in range(100):
        loss_res, _ = sess.run(
            [loss, train_op],
            feed_dict={
                x: np.random.normal(size=[5, 28, 28 ,3]),
                y: np.random.normal(size=[5, 10])})
        print(loss_res)
~~~

